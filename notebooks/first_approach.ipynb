{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLDEN_FOLD_SAMPLE_SIZE = 10_000\n",
    "RANDOM_STATE = 42\n",
    "MODELS_PRICES_SPEC = {\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"price_input_per_1K\": 0.000150,\n",
    "        \"price_output_per_1K\": 0.000600,\n",
    "        \"batch_size\": 30\n",
    "    },\n",
    "    \"gpt-4o\": {\n",
    "        \"price_input_per_1K\": 0.00250,\n",
    "        \"price_output_per_1K\": 0.01000,\n",
    "        \"batch_size\": 50\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "df_train = pd.read_csv(\"hf://datasets/yassiracharki/Amazon_Reviews_Binary_for_Sentiment_Analysis/\" + splits[\"train\"])\n",
    "df_test = pd.read_csv(\"hf://datasets/yassiracharki/Amazon_Reviews_Binary_for_Sentiment_Analysis/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_fold = df_test.sample(n=GOLDEN_FOLD_SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "golden_fold.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    Classify the sentiment of each example in the following JSON array as \"positive\" or \"negative\".\n",
    "    Respond only in JSON format where each ID is a key and its value is 1 for \"positive\" and 0 for \"negative\".\n",
    "    \n",
    "    Examples: {}\n",
    "    \n",
    "    Deliver the response here in plain text without any formatting.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_response(text):\n",
    "    if text[0]=='`':\n",
    "        text = text.replace('`','')\n",
    "        text = text.replace('json','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model, spec in tqdm(MODELS_PRICES_SPEC.items(), desc=\"Models\"):\n",
    "\n",
    "    chat_model = ChatOpenAI(model=model, temperature=0)\n",
    "\n",
    "    # Initialize variables to store results and metadata for the current model\n",
    "    model_results = {}\n",
    "    error_inputs = []\n",
    "    model_results_raw = []\n",
    "    model_total_input_tokens = []\n",
    "    model_total_output_tokens = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(0, len(golden_fold), spec[\"batch_size\"]), desc=\"Batches\"):\n",
    "        # Get the current batch of data\n",
    "        batch_data = golden_fold.iloc[i:i + spec[\"batch_size\"]]\n",
    "        \n",
    "        # Prepare the prompt\n",
    "        examples = [{\"id\": idx, \"text\": row[\"review_text\"]} for idx, row in batch_data.iterrows()]\n",
    "        input_data = prompt.format(json.dumps(examples))\n",
    "\n",
    "        response = chat_model.invoke([HumanMessage(content=input_data)])\n",
    "\n",
    "        # Clear the response text\n",
    "        response_text = clear_response(response.content)\n",
    "        try:\n",
    "            # Parse the response text as JSON\n",
    "            batch_results = json.loads(response_text)\n",
    "\n",
    "            model_total_input_tokens.append(response.usage_metadata[\"input_tokens\"])\n",
    "            model_total_output_tokens.append(response.usage_metadata[\"output_tokens\"])\n",
    "\n",
    "            model_results.update(batch_results)\n",
    "            model_results_raw.append(response_text)\n",
    "        \n",
    "        # Handle JSON decode errors and store the error inputs\n",
    "        # when the response is not a valid JSON\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error at batch {i} with error {e}\")\n",
    "            error_inputs.append([batch_data, input_data, e])\n",
    "\n",
    "    # Store the results and metadata for the current model\n",
    "    results[model] = {\n",
    "        \"results\": model_results,\n",
    "        \"total_input_tokens\": model_total_input_tokens,\n",
    "        \"total_output_tokens\": model_total_output_tokens,\n",
    "        \"total_time\": time.time() - start_time,\n",
    "        \"raw_results\": model_results_raw,\n",
    "        \"error_inputs\": error_inputs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_preds = {\n",
    "    model: results[model][\"results\"]\n",
    "    for model in results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_preds = pd.DataFrame(models_preds)\n",
    "models_preds.index = models_preds.index.astype(int)\n",
    "models_preds.columns=[f\"pred from {model}\" for model in models_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.concat([golden_fold, models_preds+1], axis=1)\n",
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if LLM makes Hallucination in observation_id, then we should remove it\n",
    "print(\"Number of hallucinations in the evaluation_df: \", evaluation_df[evaluation_df[\"class_index\"].isna()].shape[0])\n",
    "evaluation_df = evaluation_df[~evaluation_df[\"class_index\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_models = {}\n",
    "for model in MODELS_PRICES_SPEC:\n",
    "    col = f\"pred from {model}\"\n",
    "    model_data_for_metrics = evaluation_df[~evaluation_df[col].isna()]\n",
    "    metrics_by_models[model] = {\n",
    "        \"accuracy\": accuracy_score(model_data_for_metrics[\"class_index\"], model_data_for_metrics[col]),\n",
    "        \"f1\": f1_score(model_data_for_metrics[\"class_index\"], model_data_for_metrics[col])\n",
    "    }\n",
    "    print(model)\n",
    "    print(classification_report(model_data_for_metrics[\"class_index\"], model_data_for_metrics[col],target_names=[\"negative\", \"positive\"]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare results for table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS_PRICES_SPEC:\n",
    "    metrics_by_models[model][\"input_tokens_per_row\"] = sum(results[model][\"total_input_tokens\"]) / len(golden_fold)\n",
    "    metrics_by_models[model][\"output_tokens_per_row\"] = sum(results[model][\"total_output_tokens\"]) / len(golden_fold)\n",
    "    metrics_by_models[model][\"total_input_tokens\"] = sum(results[model][\"total_input_tokens\"])\n",
    "    metrics_by_models[model][\"total_output_tokens\"] = sum(results[model][\"total_output_tokens\"])\n",
    "    \n",
    "    metrics_by_models[model][\"price_per_row\"] = (\n",
    "        metrics_by_models[model][\"input_tokens_per_row\"] / 1000 * MODELS_PRICES_SPEC[model][\"price_input_per_1K\"] +\n",
    "        metrics_by_models[model][\"output_tokens_per_row\"] / 1000 * MODELS_PRICES_SPEC[model][\"price_output_per_1K\"]\n",
    "    )\n",
    "    \n",
    "    metrics_by_models[model][\"sec_per_row\"] = results[model][\"total_time\"] / len(golden_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final_table = pd.DataFrame(metrics_by_models)\n",
    "pred_final_table.loc[\"total price estimation, $\", :] = pred_final_table.loc[\"price_per_row\", :] * df_test.shape[0]\n",
    "pred_final_table.loc[\"total time estimation, hour\", :] = pred_final_table.loc[\"sec_per_row\", :] * df_test.shape[0] / 3600\n",
    "pred_final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"accuracy\", \"f1\", \"total price estimation, $\", \"total time estimation, hour\", \"total_input_tokens\", \"total_output_tokens\"]\n",
    "final_table = pred_final_table.T.reset_index(names=\"model\")\n",
    "final_table[\"approach\"]  = 1\n",
    "final_table = final_table.set_index([\"approach\", \"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table[cols].round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
