{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLD_SAMPLE_SIZE = 300_000\n",
    "NUMBER_OF_LABELS_FOR_AL = 5_000\n",
    "RANDOM_STATE = 42\n",
    "MODELS_PRICES_SPEC = {\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"price_input_per_1K\": 0.000150,\n",
    "        \"price_output_per_1K\": 0.000600,\n",
    "        \"batch_size\": 30\n",
    "    },\n",
    "    \"gpt-4o\": {\n",
    "        \"price_input_per_1K\": 0.00250,\n",
    "        \"price_output_per_1K\": 0.01000,\n",
    "        \"batch_size\": 50\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TRAIN_FOLD_SAMPLE_SIZE>=NUMBER_OF_LABELS_FOR_AL, \"TRAIN_FOLD_SAMPLE_SIZE should be greater than NUMBER_OF_LABELS_FOR_AL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/raw/ama_train.csv\")\n",
    "df_test = pd.read_csv(\"../data/raw/ama_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold_sample = df_train.sample(n=TRAIN_FOLD_SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "train_fold_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Train fold sample Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = SentenceTransformer('all-MiniLM-L6-v2', device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train_embs = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings_train = embeddings_model.encode(train_fold_sample[\"review_text\"].to_list(), show_progress_bar=True, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold_sample[\"embedding\"] = embedings_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_train_embs = time.time()-start_train_embs\n",
    "end_train_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute LLM Predictions + Active Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    Classify the sentiment of each example in the following JSON array as \"positive\" or \"negative\".\n",
    "    Respond only in JSON format where each ID is a key and its value is 1 for \"positive\" and 0 for \"negative\".\n",
    "    \n",
    "    Examples: {}\n",
    "    \n",
    "    Deliver the response here in plain text without any formatting.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_response(text):\n",
    "    if text[0]=='`':\n",
    "        text = text.replace('`','')\n",
    "        text = text.replace('json','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, spec in MODELS_PRICES_SPEC.items():\n",
    "    chat_model = ChatOpenAI(model=model, temperature=0)\n",
    "    print(\"-------------------\")\n",
    "    print(f\"Model: {model}\")\n",
    "    \n",
    "    # store labels during the active learning loop\n",
    "    labels = {}\n",
    "    # Initialize variables to store results and metadata for the current model\n",
    "    model_results = []\n",
    "    model_total_input_tokens = []\n",
    "    model_total_output_tokens = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize the active learning loop\n",
    "    initial_flag = True\n",
    "    # Initial number of examples to label\n",
    "    n_initial = spec[\"batch_size\"]\n",
    "\n",
    "    iteration = 0\n",
    "    # Error count to stop the loop if the model returns an error more than 3 times\n",
    "    error_count = 0\n",
    "    \n",
    "    # Active learning loop\n",
    "    while len(labels)<=NUMBER_OF_LABELS_FOR_AL:\n",
    "        print(f\"Iteration {iteration} Len labels: {len(labels)} \\nerror count: {error_count}\")\n",
    "        \n",
    "        # Randomly select the initial examples\n",
    "        if initial_flag:\n",
    "            np.random.seed(RANDOM_STATE)\n",
    "            query_idx = np.random.choice(train_fold_sample.index, size=n_initial, replace=False)\n",
    "            # disable initial run flag\n",
    "            initial_flag = False\n",
    "        \n",
    "\n",
    "        # Get the examples to label\n",
    "        batch_data = train_fold_sample.loc[query_idx]\n",
    "        \n",
    "        # Prepare the prompt\n",
    "        examples = [{\"id\": idx, \"text\": row[\"review_text\"]} for idx, row in batch_data.iterrows()]\n",
    "        input_data = prompt.format(json.dumps(examples))\n",
    "        \n",
    "        response = chat_model.invoke([HumanMessage(content=input_data)])\n",
    "        \n",
    "        # Clear the response text\n",
    "        response_text = clear_response(response.content)\n",
    "        \n",
    "        try:\n",
    "            # Parse the response text as JSON\n",
    "            batch_results = json.loads(response_text)\n",
    "            \n",
    "            # Filter results with incorrect structure in keys (LLM can return keys as words)\n",
    "            batch_results = {key: value for key, value in batch_results.items() if key in batch_data.index.astype(str)}\n",
    "            \n",
    "            # Store the total input and output tokens\n",
    "            model_total_input_tokens.append(response.usage_metadata[\"input_tokens\"])\n",
    "            model_total_output_tokens.append(response.usage_metadata[\"output_tokens\"])\n",
    "            model_results.append(batch_results)\n",
    "            \n",
    "            # Update the labels with the new results\n",
    "            labels.update(batch_results)\n",
    "            \n",
    "            # Prepare the data for the active learner\n",
    "            step_df_with_labels = train_fold_sample.loc[map(int, labels)]\n",
    "            step_df_with_labels[\"label\"] = [labels[str(idx)] for idx in step_df_with_labels.index]\n",
    "            \n",
    "            # Train the active learner\n",
    "            learner = ActiveLearner(\n",
    "                        estimator=LogisticRegression(),\n",
    "                        query_strategy=uncertainty_sampling,\n",
    "                        X_training=step_df_with_labels[\"embedding\"].to_list(),\n",
    "                        y_training=step_df_with_labels[\"label\"],\n",
    "                    )\n",
    "            \n",
    "            # excluding the examples that are already labeled for querying\n",
    "            step_df = train_fold_sample[~train_fold_sample.index.isin(labels)]\n",
    "            \n",
    "            # Query the next examples to label\n",
    "            query_idx_raw, query_inst = learner.query(\n",
    "                                                    step_df[\"embedding\"].to_list(), \n",
    "                                                    n_instances=n_initial\n",
    "                                                )\n",
    "            \n",
    "            # Get the index of the examples to label\n",
    "            query_idx = step_df.iloc[query_idx_raw].index\n",
    "            \n",
    "            iteration += 1\n",
    "            error_count = 0\n",
    "            \n",
    "        # Handle JSON decode errors and store the error inputs\n",
    "        # when the response is not a valid JSON\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            \n",
    "            # Stop the loop if the error count exceeds 3\n",
    "            if error_count>3:\n",
    "                print(\"Error count exceeded 3. Stopping the loop.\")\n",
    "                break\n",
    "            \n",
    "            iteration += 1\n",
    "            error_count += 1\n",
    "\n",
    "    results[model] = {\n",
    "        \"labels\": labels,\n",
    "        \"step_results\": model_results,\n",
    "        \"total_input_tokens\": model_total_input_tokens,\n",
    "        \"total_output_tokens\": model_total_output_tokens,\n",
    "        \"total_time\": time.time() - start_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare LLM + Active Learning Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_preds = {\n",
    "    model: results[model][\"labels\"]\n",
    "    for model in results\n",
    "}\n",
    "\n",
    "models_preds = pd.DataFrame(models_preds)\n",
    "models_preds.index = models_preds.index.astype(int)\n",
    "models_preds.columns=[f\"pred from {model}\" for model in models_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.concat([train_fold_sample, models_preds+1], axis=1)\n",
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_models = {}\n",
    "for model in MODELS_PRICES_SPEC:\n",
    "    col = f\"pred from {model}\"\n",
    "    data_part_for_train = evaluation_df[~evaluation_df[col].isna()]\n",
    "    model_lr = LogisticRegression(max_iter=1000)\n",
    "    model_lr.fit(data_part_for_train[\"embedding\"].to_list(), data_part_for_train[col])\n",
    "    models_models[model] = model_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Golden fold Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_for_golden_fold = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_fold = df_test.sample(n=df_test.shape[0], random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test = embeddings_model.encode(golden_fold[\"review_text\"].to_list(), show_progress_bar=True, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_fold[\"embedding\"] = embeddings_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_models = {}\n",
    "for model in MODELS_PRICES_SPEC:\n",
    "    col = f\"pred from {model}\"\n",
    "    model_lr = models_models[model]\n",
    "    golden_fold[model] = model_lr.predict(golden_fold[\"embedding\"].to_list())\n",
    "\n",
    "    metrics_by_models[model] = {\n",
    "        \"accuracy\": accuracy_score(golden_fold[\"class_index\"], golden_fold[model]),\n",
    "        \"f1\": f1_score(golden_fold[\"class_index\"], golden_fold[model])\n",
    "    }\n",
    "    print(model)\n",
    "    print(classification_report(golden_fold[\"class_index\"], golden_fold[model], target_names=[\"negative\", \"positive\"]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare results for table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS_PRICES_SPEC:\n",
    "    metrics_by_models[model][\"input_tokens_per_row\"] = sum(results[model][\"total_input_tokens\"]) / len(train_fold_sample)\n",
    "    metrics_by_models[model][\"output_tokens_per_row\"] = sum(results[model][\"total_output_tokens\"]) / len(train_fold_sample)\n",
    "    metrics_by_models[model][\"total_input_tokens\"] = sum(results[model][\"total_input_tokens\"])\n",
    "    metrics_by_models[model][\"total_output_tokens\"] = sum(results[model][\"total_output_tokens\"])\n",
    "    \n",
    "    metrics_by_models[model][\"price_per_row\"] = (\n",
    "        metrics_by_models[model][\"input_tokens_per_row\"] / 1000 * MODELS_PRICES_SPEC[model][\"price_input_per_1K\"] +\n",
    "        metrics_by_models[model][\"output_tokens_per_row\"] / 1000 * MODELS_PRICES_SPEC[model][\"price_output_per_1K\"]\n",
    "    )\n",
    "    metrics_by_models[model][\"price_total\"] = (\n",
    "        metrics_by_models[model][\"total_input_tokens\"] / 1000 * MODELS_PRICES_SPEC[model][\"price_input_per_1K\"] +\n",
    "        metrics_by_models[model][\"total_output_tokens\"] / 1000 * MODELS_PRICES_SPEC[model][\"price_output_per_1K\"]\n",
    "    )\n",
    "    metrics_by_models[model][\"total_time_annot\"] = results[model][\"total_time\"]\n",
    "    metrics_by_models[model][\"sec_per_row\"] = results[model][\"total_time\"] / len(train_fold_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final_table = pd.DataFrame(metrics_by_models)\n",
    "pred_final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time_for_golden_fold = time.time() - start_time_for_golden_fold\n",
    "pred_final_table.loc[\"total price estimation, $\", :] = pred_final_table.loc[\"price_total\", :]\n",
    "pred_final_table.loc[\"total time estimation, hour\", :] = (end_time_for_golden_fold + pred_final_table.loc[\"total_time_annot\", :] + end_train_embs) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"accuracy\", \"f1\", \"total price estimation, $\", \"total time estimation, hour\", \"total_input_tokens\", \"total_output_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = pred_final_table.T.reset_index(names=\"model\")\n",
    "final_table[\"approach\"]  = 3\n",
    "final_table = final_table.set_index([\"approach\", \"model\"])\n",
    "final_table[cols].round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
