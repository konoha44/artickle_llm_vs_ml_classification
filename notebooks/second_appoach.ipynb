{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLD_SAMPLE_SIZE = 100\n",
    "RANDOM_STATE = 42\n",
    "MODELS_PRICES_SPEC = {\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"price_input_per_1K\": 0.000150,\n",
    "        \"price_output_per_1K\": 0.000600,\n",
    "        \"batch_size\": 30\n",
    "    },\n",
    "    \"gpt-4o\": {\n",
    "        \"price_input_per_1K\": 0.00250,\n",
    "        \"price_output_per_1K\": 0.01000,\n",
    "        \"batch_size\": 50\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "df_train = pd.read_csv(\"hf://datasets/yassiracharki/Amazon_Reviews_Binary_for_Sentiment_Analysis/\" + splits[\"train\"])\n",
    "df_test = pd.read_csv(\"hf://datasets/yassiracharki/Amazon_Reviews_Binary_for_Sentiment_Analysis/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold_sample = df_train.sample(n=TRAIN_FOLD_SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "train_fold_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute LLM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    Classify the sentiment of each example in the following JSON array as \"positive\" or \"negative\".\n",
    "    Respond only in JSON format where each ID is a key and its value is 1 for \"positive\" and 0 for \"negative\".\n",
    "    \n",
    "    Examples: {}\n",
    "    \n",
    "    Deliver the response here in plain text without any formatting.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_response(text):\n",
    "    if text[0]=='`':\n",
    "        text = text.replace('`','')\n",
    "        text = text.replace('json','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model, spec in tqdm(MODELS_PRICES_SPEC.items(), desc=\"Models\"):\n",
    "\n",
    "    chat_model = ChatOpenAI(model=model, temperature=0)\n",
    "\n",
    "    # Initialize variables to store results and metadata for the current model\n",
    "    model_results = {}\n",
    "    model_results_raw = []\n",
    "    model_total_input_tokens = []\n",
    "    model_total_output_tokens = []\n",
    "    error_examples = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(0, len(train_fold_sample), spec[\"batch_size\"]), desc=\"Batches\"):\n",
    "        # Get the current batch of data\n",
    "        batch_data = train_fold_sample.iloc[i:i + spec[\"batch_size\"]]\n",
    "        \n",
    "        # Prepare the prompt\n",
    "        examples = [{\"id\": idx, \"text\": row[\"review_text\"]} for idx, row in batch_data.iterrows()]\n",
    "        input_data = prompt.format(json.dumps(examples))\n",
    "        \n",
    "        response = chat_model.invoke([HumanMessage(content=input_data)])\n",
    "\n",
    "        # Clear the response text\n",
    "        response_text = clear_response(response.content)\n",
    "        try:\n",
    "            # Parse the response text as JSON\n",
    "            batch_results = json.loads(response_text)\n",
    "            model_total_input_tokens.append(response.usage_metadata[\"input_tokens\"])\n",
    "            model_total_output_tokens.append(response.usage_metadata[\"output_tokens\"])\n",
    "            model_results.update(batch_results)\n",
    "            model_results_raw.append(response.content)\n",
    "            \n",
    "        # Handle JSON decode errors and store the error inputs\n",
    "        # when the response is not a valid JSON\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error at batch {i} with error {e}\")\n",
    "            error_examples.append([batch_data, input_data, e, response_text])\n",
    "            \n",
    "    # Store the results and metadata for the current model\n",
    "    results[model] = {\n",
    "        \"results\": model_results,\n",
    "        \"total_input_tokens\": model_total_input_tokens,\n",
    "        \"total_output_tokens\": model_total_output_tokens,\n",
    "        \"total_time\": time.time() - start_time,\n",
    "        \"raw_results\": model_results_raw,\n",
    "        \"error_examples\": error_examples\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare LLM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_preds = {\n",
    "    model: results[model][\"results\"]\n",
    "    for model in results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_preds = pd.DataFrame(models_preds)\n",
    "models_preds.index = models_preds.index.astype(int)\n",
    "models_preds.columns=[f\"pred from {model}\" for model in models_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.concat([train_fold_sample, models_preds+1], axis=1)\n",
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train fold sample Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings_model.encode(evaluation_df[\"review_text\"].to_list(), show_progress_bar=True, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df[\"embedding\"] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden Fold Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_for_golden_fold = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = df_test.sample(n=df_test.shape[0], random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = df_test.sample(n=100, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test = embeddings_model.encode(test_sample[\"review_text\"].to_list(), show_progress_bar=True, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_models = {}\n",
    "for model in MODELS_PRICES_SPEC:\n",
    "    col = f\"pred from {model}\"\n",
    "    data_part_for_train = evaluation_df[~evaluation_df[col].isna()]\n",
    "    model_lr = LogisticRegression(max_iter=1000)\n",
    "    model_lr.fit(data_part_for_train[\"embedding\"].to_list(), data_part_for_train[col])\n",
    "    models_models[model] = model_lr\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_by_models = {}\n",
    "for model in MODELS_PRICES_SPEC:\n",
    "    col = f\"pred from {model}\"\n",
    "    model_lr = models_models[model]\n",
    "    test_sample[model] = model_lr.predict(embeddings_test)\n",
    "    metrics_by_models[model] = {\n",
    "        \"accuracy\": accuracy_score(test_sample[\"class_index\"], test_sample[model]),\n",
    "        \"f1\": f1_score(test_sample[\"class_index\"], test_sample[model])\n",
    "    }\n",
    "    print(model)\n",
    "    print(classification_report(test_sample[\"class_index\"], test_sample[model], target_names=[\"negative\", \"positive\"]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare results for table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS_PRICES_SPEC:\n",
    "    metrics_by_models[model][\"input_tokens_per_row\"] = sum(results[model][\"total_input_tokens\"]) / len(train_fold_sample)\n",
    "    metrics_by_models[model][\"output_tokens_per_row\"] = sum(results[model][\"total_output_tokens\"]) / len(train_fold_sample)\n",
    "    metrics_by_models[model][\"total_input_tokens\"] = sum(results[model][\"total_input_tokens\"])\n",
    "    metrics_by_models[model][\"total_output_tokens\"] = sum(results[model][\"total_output_tokens\"])\n",
    "    \n",
    "    metrics_by_models[model][\"price_per_row\"] = (\n",
    "        metrics_by_models[model][\"input_tokens_per_row\"] / 1000 * MODELS_PRICES_SPEC[model][\"price_input_per_1K\"] +\n",
    "        metrics_by_models[model][\"output_tokens_per_row\"] / 1000 * MODELS_PRICES_SPEC[model][\"price_output_per_1K\"]\n",
    "    )\n",
    "    metrics_by_models[model][\"price_total\"] = (\n",
    "        metrics_by_models[model][\"total_input_tokens\"] / 1000 * MODELS_PRICES_SPEC[model][\"price_input_per_1K\"] +\n",
    "        metrics_by_models[model][\"total_output_tokens\"] / 1000 * MODELS_PRICES_SPEC[model][\"price_output_per_1K\"]\n",
    "    )\n",
    "    metrics_by_models[model][\"total_time_annot\"] = results[model][\"total_time\"]\n",
    "    metrics_by_models[model][\"sec_per_row\"] = results[model][\"total_time\"] / len(train_fold_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final_table = pd.DataFrame(metrics_by_models)\n",
    "pred_final_table.loc[\"total price estimation, $\", :] = pred_final_table.loc[\"price_total\", :]\n",
    "pred_final_table.loc[\"total time estimation, hour\", :] = (time.time() - start_time_for_golden_fold + pred_final_table.loc[\"total_time_annot\", :]) / 3600\n",
    "pred_final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"accuracy\", \"f1\", \"total price estimation, $\", \"total time estimation, hour\", \"total_input_tokens\", \"total_output_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = pred_final_table.T.reset_index(names=\"model\")\n",
    "final_table[\"approach\"]  = 2\n",
    "final_table = final_table.set_index([\"approach\", \"model\"])\n",
    "final_table[cols].round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
